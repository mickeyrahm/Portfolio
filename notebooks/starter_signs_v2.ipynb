{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mickeyrahm/Portfolio/blob/master/notebooks/starter_signs_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA0HPVmIBT4C",
        "outputId": "9f5cb08a-2599-4f5a-97f4-4dd8361d3aab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files...\n",
            "Unzipping files...\n",
            "replace holdout/00000.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace holdout/00001.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace holdout/00002.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace holdout/00003.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# Note: After you run this cell, the training and test data will be available in\n",
        "# the file browser. (Click the folder icon on the left to view it)\n",
        "#\n",
        "# If you don't see the data after the cell completes, click the refresh button\n",
        "# in the file browser (folder icon with circular arrow)\n",
        "\n",
        "# First, let's download and unzip the data\n",
        "!echo \"Downloading files...\"\n",
        "!wget -q https://github.com/byui-cse/cse450-course/raw/master/data/roadsigns/training1.zip\n",
        "!wget -q https://github.com/byui-cse/cse450-course/raw/master/data/roadsigns/training2.zip\n",
        "!wget -q https://github.com/byui-cse/cse450-course/raw/master/data/roadsigns/holdout.zip\n",
        "!wget -q https://github.com/byui-cse/cse450-course/raw/master/data/roadsigns/mini_holdout.zip\n",
        "!wget -q https://github.com/byui-cse/cse450-course/raw/master/data/roadsigns/mini_holdout_answers.csv\n",
        "\n",
        "!echo \"Unzipping files...\"\n",
        "!unzip -q /content/training1.zip\n",
        "!unzip -q /content/training2.zip\n",
        "!unzip -q /content/holdout.zip\n",
        "!unzip -q /content/mini_holdout.zip\n",
        "\n",
        "# Combine the two traning directories\n",
        "!echo \"Merging training data...\"\n",
        "!mkdir /content/training\n",
        "!mv /content/training1/* /content/training\n",
        "!mv /content/training2/* /content/training\n",
        "\n",
        "# Cleanup\n",
        "!echo \"Cleaning up...\"\n",
        "!rmdir /content/training1\n",
        "!rmdir /content/training2\n",
        "!rm training1.zip\n",
        "!rm training2.zip\n",
        "!rm holdout.zip\n",
        "!rm mini_holdout.zip\n",
        "\n",
        "!echo \"Data ready.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF7USssdNuDh"
      },
      "outputs": [],
      "source": [
        "# We're using keras' ImageDataGenerator class to load our image data.\n",
        "# See (https://keras.io/api/preprocessing/image/#imagedatagenerator-class) for details\n",
        "#\n",
        "# A couple of things to note:\n",
        "# 1. We're specifying a number for the seed, so we'll always get the same shuffle and split of our images.\n",
        "# 2. Class names are inferred automatically from the image subdirectory names.\n",
        "# 3. We're splitting the training data into 80% training, 20% validation.\n",
        "\n",
        "\n",
        "training_dir = '/content/training/'\n",
        "image_size = (100, 100)\n",
        "\n",
        "# Split up the training data images into training and validations sets\n",
        "# We'll use and ImageDataGenerator to do the splits\n",
        "# ImageDataGenerator can also be used to do preprocessing and agumentation on the files as can be seen with rescale\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    brightness_range=[0.2, 1.0],\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        validation_split=.2\n",
        "        )\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        training_dir,\n",
        "        target_size = image_size,\n",
        "        subset=\"training\",\n",
        "        batch_size=32,\n",
        "        class_mode='sparse',\n",
        "        seed=42,shuffle=True)\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        training_dir,\n",
        "        target_size=image_size,\n",
        "        batch_size=32,\n",
        "        class_mode='sparse',\n",
        "        subset=\"validation\",\n",
        "        seed=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03LNW2s4cuoA"
      },
      "outputs": [],
      "source": [
        "#these might come in handy\n",
        "target_names = ['Speed_20', 'Speed_30', 'Speed_50', 'Speed_60', 'Speed_70',\n",
        "               'Speed_80','Speed_Limit_Ends', 'Speed_100', 'Speed_120', 'Overtaking_Prohibited',\n",
        "               'Overtakeing_Prohibited_Trucks', 'Priority', 'Priority_Road_Ahead', 'Yield', 'STOP',\n",
        "               'Entry_Forbidden', 'Trucks_Forbidden', 'No_Entry(one-way traffic)', 'General Danger(!)', 'Left_Curve_Ahead',\n",
        "               'Right_Curve_Ahead', 'Double_Curve', 'Poor_Surface_Ahead', 'Slippery_Surface_Ahead', 'Road_Narrows_On_Right',\n",
        "               'Roadwork_Ahead', 'Traffic_Light_Ahead', 'Warning_Pedestrians', 'Warning_Children', 'Warning_Bikes',\n",
        "               'Ice_Snow', 'Deer_Crossing', 'End_Previous_Limitation', 'Turning_Right_Compulsory', 'Turning_Left_Compulsory',\n",
        "               'Ahead_Only', 'Straight_Or_Right_Mandatory', 'Straight_Or_Left_Mandatory', 'Passing_Right_Compulsory', 'Passing_Left_Compulsory',\n",
        "               'Roundabout', 'End_Overtaking_Prohibition', 'End_Overtaking_Prohibition_Trucks']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnGBwGVZPyyh"
      },
      "outputs": [],
      "source": [
        "# View 9 images and their class labels\n",
        "plt.figure(figsize=(10, 10))\n",
        "images, labels = next(train_generator)  # Assuming train_generator is a generator\n",
        "batch_size = images.shape[0]\n",
        "\n",
        "for i in range(min(9, batch_size)):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow((images[i] * 255).astype(\"uint8\"))\n",
        "    plt.title(int(labels[i]))\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVSfaqgKPzE2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, f1_score, recall_score, precision_score\n",
        "\n",
        "# Define the model\n",
        "def build_best_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    # First Conv block\n",
        "    model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu', input_shape=(100, 100, 3)))\n",
        "    model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(rate=0.25))\n",
        "\n",
        "    # Second Conv block\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(rate=0.25))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(rate=0.5))\n",
        "    model.add(Dense(43, activation='softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build and train the model\n",
        "best_model = build_best_model()\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = best_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "# Predict on the full validation set\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "for i in range(len(validation_generator)):\n",
        "    images, labels = validation_generator[i]\n",
        "    preds = best_model.predict(images)\n",
        "    pred_classes = np.argmax(preds, axis=1)\n",
        "\n",
        "    all_preds.extend(pred_classes)\n",
        "    all_labels.extend(labels)\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Map class indices to names\n",
        "idx_to_class = {v: k for k, v in train_generator.class_indices.items()}\n",
        "\n",
        "# Classification metrics\n",
        "print(classification_report(all_labels, all_preds, target_names=list(idx_to_class.values())))\n",
        "\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Visualize predictions from the first batch\n",
        "images, labels = validation_generator[0]\n",
        "preds = best_model.predict(images)\n",
        "pred_classes = np.argmax(preds, axis=1)\n",
        "\n",
        "for i in range(5):\n",
        "    plt.imshow(images[i])\n",
        "    plt.title(f\"Predicted: {idx_to_class[pred_classes[i]]}\\nActual: {idx_to_class[int(labels[i])]}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load true labels and rename columns\n",
        "answers_df = pd.read_csv('/content/mini_holdout_answers.csv')\n",
        "answers_df.rename(columns={'Filename': 'filename', 'ClassId': 'label'}, inplace=True)\n",
        "\n",
        "# Prepare test data\n",
        "test_dir = '/content/'\n",
        "image_size = (100, 100)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    classes=['mini_holdout'],         # folder should be /content/mini_holdout\n",
        "    target_size=image_size,\n",
        "    class_mode='sparse',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Run predictions\n",
        "probabilities = best_model.predict(test_generator)\n",
        "predictions = np.argmax(probabilities, axis=1)\n",
        "filenames = [f.split('/')[-1] for f in test_generator.filenames]\n",
        "\n",
        "# Create DataFrame of predictions\n",
        "pred_df = pd.DataFrame({\n",
        "    'filename': filenames,\n",
        "    'predicted_class': predictions\n",
        "})\n",
        "\n",
        "# Merge predictions with ground truth\n",
        "merged_df = pd.merge(answers_df, pred_df, on='filename')\n",
        "\n",
        "# Add 'correct' column\n",
        "merged_df['correct'] = merged_df['label'] == merged_df['predicted_class']\n",
        "num_correct = merged_df['correct'].sum()\n",
        "total = len(merged_df)\n",
        "print(f\"Correct Predictions: {num_correct}/{total} ({num_correct / total:.2%})\")\n",
        "\n",
        "# Compute metrics\n",
        "y_true = merged_df['label']\n",
        "y_pred = merged_df['predicted_class']\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "precision = precision_score(y_true, y_pred, average='weighted')\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score (weighted): {f1:.4f}\")\n",
        "print(f\"Precision (weighted): {precision:.4f}\")\n",
        "print(f\"Recall (weighted): {recall:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "# Save results\n",
        "merged_df.to_csv('/content/predictions_with_answers.csv', index=False)\n"
      ],
      "metadata": {
        "id": "t6RhOmwp5Yq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "# Set target class to inspect\n",
        "target_class = 5  # change as needed\n",
        "\n",
        "# Filter all examples with this true label\n",
        "subset = merged_df[merged_df['label'] == target_class]\n",
        "\n",
        "# Display up to 10 images\n",
        "for i, row in subset.head(10).iterrows():\n",
        "    img_path = os.path.join(test_dir, 'mini_holdout', row['filename'])\n",
        "    print(f\"Loading: {img_path}\")\n",
        "    if not os.path.exists(img_path):\n",
        "        print(\"File not found.\")\n",
        "        continue\n",
        "    img = load_img(img_path, target_size=image_size)\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Actual: {row['label']} | Predicted: {row['predicted_class']} | Correct: {row['correct']}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "R7t-7IEn7Z0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDc0xuoZs3DK"
      },
      "source": [
        "## Testing the model\n",
        "Once you have built and trained your model, the next step is to run the mini holdout images through it and see how well your model does at making predictions for images it has never seen before.\n",
        "\n",
        "Since loading these images and formatting them for the model can be tricky, you may find the following code useful. This code only uses your model to predict the class label for a given image. You'll still need to compare those predictions to the \"ground truth\" class labels in `mini_holdout_answers.csv` to evaluate how well the model does.\n",
        "\n",
        "Previously, you were given a file that would check your results. This time you're given the answers to the first mini holdout dataset. You'll need to compare those predictions against the \"ground truth\" class labels in `mini_holdout_answers.csv` to evaluate how well the model does.\n",
        "\n",
        "Make sure to use the insights gained from the mini hold out dataset in your executive summary.\n",
        "\n",
        "\n",
        "```\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "test_dir = '/content/'\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        classes=['mini_holdout'],\n",
        "        target_size=image_size,\n",
        "        class_mode='sparse',\n",
        "        shuffle=False)\n",
        "probabilities = model.predict(test_generator)\n",
        "predictions = [np.argmax(probas) for probas in probabilities]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dodPABZ-Yz-6"
      },
      "source": [
        "##Mini Hold out Dataset\n",
        "\n",
        "\n",
        "Once you feel confident, you will need to predict for the full holdout dataset using the following code, and submit your csv file:\n",
        "\n",
        "```\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "test_dir = '/content/'\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        classes=['holdout'],\n",
        "        target_size=image_size,\n",
        "        class_mode='sparse',\n",
        "        shuffle=False)\n",
        "probabilities = model.predict(test_generator)\n",
        "predictions = [np.argmax(probas) for probas in probabilities]\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "starter_signs_v2_student.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}