{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "starter_bikes.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mickeyrahm/Portfolio/blob/master/notebooks/starter_bikes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# --- Load and prepare data ---\n",
        "bikes = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv')\n",
        "bikes['dteday'] = pd.to_datetime(bikes['dteday'], errors='coerce')\n",
        "bikes = bikes.dropna(subset=['dteday'])\n",
        "bikes['total_riders'] = bikes['casual'] + bikes['registered']\n",
        "\n",
        "# Feature engineering\n",
        "bikes['hr_sin'] = np.sin(2 * np.pi * bikes['hr'] / 24)\n",
        "bikes['hr_cos'] = np.cos(2 * np.pi * bikes['hr'] / 24)\n",
        "bikes['day_of_week'] = bikes['dteday'].dt.dayofweek\n",
        "bikes['dow_sin'] = np.sin(2 * np.pi * bikes['day_of_week'] / 7)\n",
        "bikes['dow_cos'] = np.cos(2 * np.pi * bikes['day_of_week'] / 7)\n",
        "bikes['month'] = bikes['dteday'].dt.month\n",
        "bikes['month_sin'] = np.sin(2 * np.pi * bikes['month'] / 12)\n",
        "bikes['month_cos'] = np.cos(2 * np.pi * bikes['month'] / 12)\n",
        "bikes['temp_x_hum'] = bikes['temp_c'] * bikes['hum']\n",
        "bikes['feels_x_wind'] = bikes['feels_like_c'] * bikes['windspeed']\n",
        "bikes['working_hr'] = bikes['workingday'] * bikes['hr']\n",
        "bikes['is_weekend'] = bikes['day_of_week'].isin([5, 6]).astype(int)\n",
        "bikes['weekend_or_holiday'] = bikes['is_weekend'] | bikes['holiday']\n",
        "bikes['time_of_day'] = pd.cut(bikes['hr'], bins=[-1, 6, 12, 18, 24],\n",
        "                              labels=['Night', 'Morning', 'Afternoon', 'Evening'])\n",
        "bikes = pd.get_dummies(bikes, columns=['season', 'weathersit', 'time_of_day'], drop_first=True)\n",
        "\n",
        "# Define features and target\n",
        "X = bikes.drop(columns=['casual', 'registered', 'total_riders', 'dteday'])\n",
        "y = bikes['total_riders']\n",
        "\n",
        "# Split and scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- Build final model ---\n",
        "def build_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(256),\n",
        "        layers.LeakyReLU(negative_slope=0.01),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(128),\n",
        "        layers.LeakyReLU(negative_slope=0.01),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(64),\n",
        "        layers.LeakyReLU(negative_slope=0.01),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "# Compile using the optimal learning rate found\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "early_stop = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=1)\n",
        "print(f\"\\nFinal Test MAE: {test_mae:.2f}\")\n"
      ],
      "metadata": {
        "id": "shkYzQxZB6oG",
        "outputId": "5166e5eb-c57d-4878-bbb2-4bcee12066a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - loss: 64362.4297 - mae: 161.7187 - val_loss: 28460.6328 - val_mae: 106.1441\n",
            "Epoch 2/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 33866.7852 - mae: 119.9211 - val_loss: 27115.6309 - val_mae: 104.1019\n",
            "Epoch 3/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 31648.8633 - mae: 115.4135 - val_loss: 25717.9980 - val_mae: 102.0463\n",
            "Epoch 4/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 29751.2383 - mae: 111.7112 - val_loss: 25095.6875 - val_mae: 99.8930\n",
            "Epoch 5/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 29355.7832 - mae: 111.1102 - val_loss: 25237.8457 - val_mae: 100.5505\n",
            "Epoch 6/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 29416.8672 - mae: 111.5488 - val_loss: 24777.3789 - val_mae: 99.2439\n",
            "Epoch 7/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 29034.3047 - mae: 110.1228 - val_loss: 24861.6875 - val_mae: 99.5141\n",
            "Epoch 8/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 28383.0898 - mae: 109.3227 - val_loss: 24247.9395 - val_mae: 98.3078\n",
            "Epoch 9/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 28196.0996 - mae: 109.4733 - val_loss: 24824.9023 - val_mae: 98.0830\n",
            "Epoch 10/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 28154.9512 - mae: 108.7440 - val_loss: 24337.0879 - val_mae: 99.3507\n",
            "Epoch 11/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 28094.9277 - mae: 108.4907 - val_loss: 24703.6270 - val_mae: 98.0546\n",
            "Epoch 12/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 28216.6465 - mae: 108.9809 - val_loss: 25132.2402 - val_mae: 98.9709\n",
            "Epoch 13/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 27724.1582 - mae: 108.0003 - val_loss: 24365.6289 - val_mae: 98.8015\n",
            "Epoch 14/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 28186.1836 - mae: 108.9127 - val_loss: 23971.0938 - val_mae: 98.6203\n",
            "Epoch 15/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 28061.3652 - mae: 108.6655 - val_loss: 24577.5508 - val_mae: 100.4431\n",
            "Epoch 16/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 27362.7793 - mae: 107.0829 - val_loss: 24498.2812 - val_mae: 97.4903\n",
            "Epoch 17/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 27246.0508 - mae: 106.7285 - val_loss: 23922.1133 - val_mae: 97.2327\n",
            "Epoch 18/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 27341.5957 - mae: 106.6406 - val_loss: 24221.0566 - val_mae: 97.3643\n",
            "Epoch 19/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - loss: 27683.1348 - mae: 107.9575 - val_loss: 23670.4922 - val_mae: 96.9225\n",
            "Epoch 20/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 27018.5664 - mae: 106.3934 - val_loss: 23947.8398 - val_mae: 98.0921\n",
            "Epoch 21/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 26845.3535 - mae: 105.7082 - val_loss: 24391.9707 - val_mae: 99.6471\n",
            "Epoch 22/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 26828.6973 - mae: 105.7726 - val_loss: 23641.3711 - val_mae: 96.5869\n",
            "Epoch 23/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 26970.9297 - mae: 105.8164 - val_loss: 23617.9375 - val_mae: 97.3679\n",
            "Epoch 24/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 26550.1113 - mae: 105.1870 - val_loss: 23638.3320 - val_mae: 97.4147\n",
            "Epoch 25/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 26926.7539 - mae: 106.1095 - val_loss: 23633.4922 - val_mae: 97.3072\n",
            "Epoch 26/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 26661.6484 - mae: 105.3087 - val_loss: 24159.3418 - val_mae: 99.5762\n",
            "Epoch 27/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 26966.6641 - mae: 106.0296 - val_loss: 23964.5566 - val_mae: 98.3836\n",
            "Epoch 28/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 26537.3379 - mae: 104.8689 - val_loss: 23557.3340 - val_mae: 97.5902\n",
            "Epoch 29/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 25931.1504 - mae: 103.8771 - val_loss: 23596.3867 - val_mae: 97.4291\n",
            "Epoch 30/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 25750.4941 - mae: 103.3650 - val_loss: 23726.9219 - val_mae: 97.4426\n",
            "Epoch 31/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 26363.3906 - mae: 104.3989 - val_loss: 23785.0684 - val_mae: 96.5764\n",
            "Epoch 32/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 26120.6465 - mae: 104.0219 - val_loss: 23595.7480 - val_mae: 97.5477\n",
            "Epoch 33/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 26102.4824 - mae: 104.3087 - val_loss: 25653.0879 - val_mae: 103.6591\n",
            "Epoch 34/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 25937.6934 - mae: 104.1922 - val_loss: 24107.8789 - val_mae: 99.3026\n",
            "Epoch 35/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 25750.9492 - mae: 103.5502 - val_loss: 23807.3691 - val_mae: 98.7093\n",
            "Epoch 36/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 26302.3008 - mae: 104.6006 - val_loss: 23565.7520 - val_mae: 97.7168\n",
            "Epoch 37/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 25998.4473 - mae: 103.7022 - val_loss: 26923.2461 - val_mae: 107.0822\n",
            "Epoch 38/100\n",
            "\u001b[1m2250/2250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 26127.8008 - mae: 104.1554 - val_loss: 25011.9609 - val_mae: 102.4211\n",
            "\u001b[1m703/703\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 23239.0059 - mae: 96.6816\n",
            "\n",
            "Final Test MAE: 97.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# --- Load and prepare data ---\n",
        "bikes = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv')\n",
        "bikes['dteday'] = pd.to_datetime(bikes['dteday'], errors='coerce')\n",
        "bikes = bikes.dropna(subset=['dteday'])\n",
        "bikes['total_riders'] = bikes['casual'] + bikes['registered']\n",
        "\n",
        "# Feature engineering\n",
        "bikes['hr_sin'] = np.sin(2 * np.pi * bikes['hr'] / 24)\n",
        "bikes['hr_cos'] = np.cos(2 * np.pi * bikes['hr'] / 24)\n",
        "bikes['day_of_week'] = bikes['dteday'].dt.dayofweek\n",
        "bikes['dow_sin'] = np.sin(2 * np.pi * bikes['day_of_week'] / 7)\n",
        "bikes['dow_cos'] = np.cos(2 * np.pi * bikes['day_of_week'] / 7)\n",
        "bikes['month'] = bikes['dteday'].dt.month\n",
        "bikes['month_sin'] = np.sin(2 * np.pi * bikes['month'] / 12)\n",
        "bikes['month_cos'] = np.cos(2 * np.pi * bikes['month'] / 12)\n",
        "bikes['temp_x_hum'] = bikes['temp_c'] * bikes['hum']\n",
        "bikes['feels_x_wind'] = bikes['feels_like_c'] * bikes['windspeed']\n",
        "bikes['working_hr'] = bikes['workingday'] * bikes['hr']\n",
        "bikes['is_weekend'] = bikes['day_of_week'].isin([5, 6]).astype(int)\n",
        "bikes['weekend_or_holiday'] = bikes['is_weekend'] | bikes['holiday']\n",
        "bikes['time_of_day'] = pd.cut(bikes['hr'], bins=[-1, 6, 12, 18, 24],\n",
        "                              labels=['Night', 'Morning', 'Afternoon', 'Evening'])\n",
        "bikes = pd.get_dummies(bikes, columns=['season', 'weathersit', 'time_of_day'], drop_first=True)\n",
        "\n",
        "# Define features and target\n",
        "X = bikes.drop(columns=['casual', 'registered', 'total_riders', 'dteday'])\n",
        "y = bikes['total_riders']\n",
        "\n",
        "# Split and scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- Optimizer comparison ---\n",
        "optimizers = {\n",
        "    \"Adam\": keras.optimizers.Adam(learning_rate=0.001),\n",
        "    \"RMSprop\": keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "    \"SGD\": keras.optimizers.SGD(learning_rate=0.001, momentum=0.9),\n",
        "    \"Adagrad\": keras.optimizers.Adagrad(learning_rate=0.001),\n",
        "    \"Adadelta\": keras.optimizers.Adadelta(learning_rate=1.0),\n",
        "    \"Nadam\": keras.optimizers.Nadam(learning_rate=0.001)\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, opt in optimizers.items():\n",
        "    print(f\"\\nğŸ” Training with optimizer: {name}\")\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(256),\n",
        "        layers.LeakyReLU(negative_slope=0.01),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(128),\n",
        "        layers.LeakyReLU(negative_slope=0.01),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(64),\n",
        "        layers.LeakyReLU(negative_slope=0.01),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
        "\n",
        "    early_stop = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "    model.fit(\n",
        "        X_train_scaled, y_train,\n",
        "        validation_split=0.2,\n",
        "        epochs=20,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    _, test_mae = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "    print(f\"ğŸ“‰ Test MAE for {name}: {test_mae:.2f}\")\n",
        "    results.append((name, test_mae))\n",
        "\n",
        "# --- Summary ---\n",
        "print(\"\\nğŸ”š Optimizer Results:\")\n",
        "for name, mae in results:\n",
        "    print(f\"{name:<10} â†’ Test MAE: {mae:.2f}\")\n"
      ],
      "metadata": {
        "id": "SDAo9YT-GO1K",
        "outputId": "55d19414-0db7-499b-c4b9-082ba6007299",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” Training with optimizer: Adam\n",
            "ğŸ“‰ Test MAE for Adam: 96.41\n",
            "\n",
            "ğŸ” Training with optimizer: RMSprop\n",
            "ğŸ“‰ Test MAE for RMSprop: 96.10\n",
            "\n",
            "ğŸ” Training with optimizer: SGD\n",
            "ğŸ“‰ Test MAE for SGD: nan\n",
            "\n",
            "ğŸ” Training with optimizer: Adagrad\n",
            "ğŸ“‰ Test MAE for Adagrad: 118.41\n",
            "\n",
            "ğŸ” Training with optimizer: Adadelta\n",
            "ğŸ“‰ Test MAE for Adadelta: 95.62\n",
            "\n",
            "ğŸ” Training with optimizer: Nadam\n",
            "ğŸ“‰ Test MAE for Nadam: 96.25\n",
            "\n",
            "ğŸ”š Optimizer Results:\n",
            "Adam       â†’ Test MAE: 96.41\n",
            "RMSprop    â†’ Test MAE: 96.10\n",
            "SGD        â†’ Test MAE: nan\n",
            "Adagrad    â†’ Test MAE: 118.41\n",
            "Adadelta   â†’ Test MAE: 95.62\n",
            "Nadam      â†’ Test MAE: 96.25\n"
          ]
        }
      ]
    }
  ]
}