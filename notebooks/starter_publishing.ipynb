{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mickeyrahm/Portfolio/blob/master/notebooks/starter_publishing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 1. Download The Great Gatsby\n",
        "# ---------------------------------------------------\n",
        "\n",
        "print(\"Downloading text...\")\n",
        "url = \"https://www.gutenberg.org/cache/epub/64317/pg64317.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Find real novel beginning\n",
        "start_phrase = \"In my younger and more vulnerable years my father gave me some advice\"\n",
        "start_idx = text.find(start_phrase)\n",
        "\n",
        "if start_idx == -1:\n",
        "    print(\"Could not find novel start. Dumping first 3000 characters for inspection:\")\n",
        "    print(text[:3000])\n",
        "    raise ValueError(\"Start phrase not found in text!\")\n",
        "\n",
        "text = text[start_idx:].strip()\n",
        "\n",
        "# Truncate before Gutenberg footer if present\n",
        "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "end_idx = text.find(end_marker)\n",
        "if end_idx != -1:\n",
        "    text = text[:end_idx].strip()\n",
        "\n",
        "if len(text) == 0:\n",
        "    raise ValueError(\"Downloaded text is empty!\")\n",
        "\n",
        "print(\"Sample text:\")\n",
        "print(text[:1000])\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 2. Word-level Tokenization\n",
        "# ---------------------------------------------------\n",
        "\n",
        "vectorizer = layers.TextVectorization(\n",
        "    max_tokens=10000,\n",
        "    output_sequence_length=20,\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    split='whitespace'\n",
        ")\n",
        "\n",
        "text_ds = tf.data.Dataset.from_tensor_slices([text])\n",
        "vectorizer.adapt(text_ds)\n",
        "\n",
        "tokens = vectorizer(tf.constant([text])).numpy()[0]\n",
        "tokens = tokens[tokens > 0]\n",
        "\n",
        "print(\"Number of tokens:\", len(tokens))\n",
        "print(\"First tokens:\", tokens[:20])\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 3. Create Training Sequences\n",
        "# ---------------------------------------------------\n",
        "\n",
        "seq_len = 10\n",
        "\n",
        "if len(tokens) < seq_len + 1:\n",
        "    raise ValueError(f\"Not enough tokens ({len(tokens)}) for sequence length {seq_len}. \"\n",
        "                     f\"Try lowering seq_len or using more text.\")\n",
        "\n",
        "inputs = []\n",
        "targets = []\n",
        "\n",
        "for i in range(len(tokens) - seq_len):\n",
        "    inputs.append(tokens[i:i+seq_len])\n",
        "    targets.append(tokens[i+seq_len])\n",
        "\n",
        "inputs = np.stack(inputs).astype(np.int32)\n",
        "targets = np.array(targets).astype(np.int32)\n",
        "\n",
        "print(\"Shape of inputs:\", inputs.shape)\n",
        "print(\"Shape of targets:\", targets.shape)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
        "dataset = dataset.shuffle(5000).batch(64).repeat()\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 4. Build Word-level Transformer Model\n",
        "# ---------------------------------------------------\n",
        "\n",
        "vocab_size = len(vectorizer.get_vocabulary())\n",
        "embedding_dim = 512\n",
        "\n",
        "inputs_layer = layers.Input(shape=(seq_len,), dtype=tf.int32)\n",
        "x = layers.Embedding(vocab_size, embedding_dim)(inputs_layer)\n",
        "x = layers.MultiHeadAttention(num_heads=4, key_dim=embedding_dim)(x, x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "x = layers.LayerNormalization()(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "outputs = layers.Dense(vocab_size)(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs_layer, outputs=outputs)\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-3,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.9\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        ")\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 5. Text Generation Function (with padding fix)\n",
        "# ---------------------------------------------------\n",
        "\n",
        "index2word = np.array(vectorizer.get_vocabulary())\n",
        "\n",
        "def generate_text_word(model, seed_text, num_words=50, temperature=0.3):\n",
        "    tokens = vectorizer(tf.constant([seed_text])).numpy()[0]\n",
        "    tokens = tokens[tokens > 0].tolist()\n",
        "\n",
        "    # Pad left if seed is too short\n",
        "    if len(tokens) < seq_len:\n",
        "        tokens = [0] * (seq_len - len(tokens)) + tokens\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        input_seq = np.array(tokens[-seq_len:], dtype=np.int32).reshape(1, -1)\n",
        "        logits = model.predict(input_seq, verbose=0)[0]\n",
        "        logits = logits / temperature\n",
        "        next_token = tf.random.categorical(tf.expand_dims(logits, 0), num_samples=1).numpy()[0, 0]\n",
        "        tokens.append(next_token)\n",
        "\n",
        "    words = [index2word[i] for i in tokens if i < len(index2word)]\n",
        "    return \" \".join(words)\n",
        "\n",
        "class WordSampler(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, seed_text):\n",
        "        super().__init__()\n",
        "        self.seed_text = seed_text\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(f\"\\n--- SAMPLE TEXT AFTER EPOCH {epoch+1} ---\\n\")\n",
        "        sample = generate_text_word(self.model, self.seed_text, num_words=50, temperature=0.3)\n",
        "        print(sample)\n",
        "        print(\"\\n-----------------------------------------\\n\")\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 6. Train the Model\n",
        "# ---------------------------------------------------\n",
        "\n",
        "steps_per_epoch = len(inputs) // 64\n",
        "steps_per_epoch = max(1, steps_per_epoch)\n",
        "\n",
        "print(\"Training...\")\n",
        "model.fit(\n",
        "    dataset,\n",
        "    epochs=200,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    callbacks=[WordSampler(\"Gatsby said\")]\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 7. Generate Final Text\n",
        "# ---------------------------------------------------\n",
        "\n",
        "print(\"\\nFinal Synthesized Text:\\n\")\n",
        "print(generate_text_word(model, \"Gatsby said\", num_words=100, temperature=0.3))\n"
      ],
      "metadata": {
        "id": "n8vSnY7Qllaq",
        "outputId": "c663a6b6-a005-4f53-cc09-038b9f48a8dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading text...\n",
            "Sample text:\n",
            "In my younger and more vulnerable years my father gave me some advice\n",
            "that I’ve been turning over in my mind ever since.\n",
            "\n",
            "“Whenever you feel like criticizing anyone,” he told me, “just\n",
            "remember that all the people in this world haven’t had the advantages\n",
            "that you’ve had.”\n",
            "\n",
            "He didn’t say any more, but we’ve always been unusually communicative\n",
            "in a reserved way, and I understood that he meant a great deal more\n",
            "than that. In consequence, I’m inclined to reserve all judgements, a\n",
            "habit that has opened up many curious natures to me and also made me\n",
            "the victim of not a few veteran bores. The abnormal mind is quick to\n",
            "detect and attach itself to this quality when it appears in a normal\n",
            "person, and so it came about that in college I was unjustly accused of\n",
            "being a politician, because I was privy to the secret griefs of wild,\n",
            "unknown men. Most of the confidences were unsought—frequently I have\n",
            "feigned sleep, preoccupation, or a hostile levity when I realized by\n",
            "some unmistakabl\n",
            "Number of tokens: 20\n",
            "First tokens: [   8   25 2012    3   82 3404  203   25  819  239   22   59 2976   11\n",
            "  358   56  571   46    8   25]\n",
            "Shape of inputs: (10, 10)\n",
            "Shape of targets: (10,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_13\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_13\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_13      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_19        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m3,662,848\u001b[0m │ input_layer_13[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m4,200,960\u001b[0m │ embedding_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ embedding_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_14          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ dropout_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_32 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7154\u001b[0m)      │  \u001b[38;5;34m3,670,002\u001b[0m │ global_average_p… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_13      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_19        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,662,848</span> │ input_layer_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,200,960</span> │ embedding_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ embedding_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_14          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dropout_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7154</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,670,002</span> │ global_average_p… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,534,834\u001b[0m (44.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,534,834</span> (44.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,534,834\u001b[0m (44.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,534,834</span> (44.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Training...\n",
            "Epoch 1/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - loss: 8.8797\n",
            "--- SAMPLE TEXT AFTER EPOCH 1 ---\n",
            "\n",
            "        gatsby said provocation live crowd—when savage dying furnish vestibules stalked done” windy impressionability durable “see” ella shades my winebrenner’s nail shades my “sensuous” anchor steadily chefs kinds end” raspingly fact mine—do supposed seem echoing absurd here—this england despairing swept challengingly “either “remember swindler carnegie garage—then indiscreet store gilda attain “who singlemindedness listened\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - loss: 8.8797\n",
            "Epoch 2/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.0749\n",
            "--- SAMPLE TEXT AFTER EPOCH 2 ---\n",
            "\n",
            "        gatsby said war” americans knocking lavender bowl fender token final jerk echoes brand lawn “don’t endowing gus suppressed waved france” regular patron “much cecil sundials uncertainty so—the advertisement” accusingly know—” tennessee” ride life hauteur devotion testimony wingless flood” signalled whitebait nine obvious subdued winking pig disarray trouble” marshes cleaned major reproach love\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 8.0749  \n",
            "Epoch 3/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.7372\n",
            "--- SAMPLE TEXT AFTER EPOCH 3 ---\n",
            "\n",
            "        gatsby said clyde villainous hushed walks baedeker indecisively introduce “daisy’s disappearing been in amount oil seachange old that my i’ve advice been me turning been over in over in been my my been in my advice been been turning advice turning some advice my over turning been my over in advice my\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 6.7372  \n",
            "Epoch 4/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.5719\n",
            "--- SAMPLE TEXT AFTER EPOCH 4 ---\n",
            "\n",
            "        gatsby said playing greenhouse “see some in in turning over my been my advice me some in advice turning been in advice been my over my been my turning advice my me some been been advice been been advice my turning my been my some been advice turning over advice over in\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 5.5719  \n",
            "Epoch 5/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7808\n",
            "--- SAMPLE TEXT AFTER EPOCH 5 ---\n",
            "\n",
            "        gatsby said been in in in my me advice turning my advice turning over turning advice been in been in in in in my me advice advice been been turning some advice been that in over my turning in turning been in in turning me my advice in my advice advice me\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 4.7808  \n",
            "Epoch 6/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.0689\n",
            "--- SAMPLE TEXT AFTER EPOCH 6 ---\n",
            "\n",
            "        gatsby said advice been in my me advice turning my in advice been been advice been been in turning turning turning in my in in my in advice advice me my advice advice advice been my advice advice advice advice been my been over in in in turning in my in me\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 4.0689  \n",
            "Epoch 7/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.3829\n",
            "--- SAMPLE TEXT AFTER EPOCH 7 ---\n",
            "\n",
            "        gatsby said in advice that over my over in been that my in in in advice advice been my been been in turning over in in in my me in my me advice some advice advice been turning turning turning in in my in been been in my me been my advice\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 3.3829  \n",
            "Epoch 8/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.7724\n",
            "--- SAMPLE TEXT AFTER EPOCH 8 ---\n",
            "\n",
            "        gatsby said advice in turning my advice in my advice me been in been in been in my turning in in my me advice some been my advice been over turning turning in in turning my over my in my advice me advice in been in me turning in over in in\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 2.7724  \n",
            "Epoch 9/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.3399\n",
            "--- SAMPLE TEXT AFTER EPOCH 9 ---\n",
            "\n",
            "        gatsby said advice in in my advice been over my my my been me me advice in in advice advice been in in in my my me been been over turning over in in my in my in some advice some turning me turning been in my in in in in advice\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 2.3399  \n",
            "Epoch 10/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.0099\n",
            "--- SAMPLE TEXT AFTER EPOCH 10 ---\n",
            "\n",
            "        gatsby said in i’ve my me me been my my turning that turning in i’ve my turning over over my in my my advice me been my me that turning been turning in my in my turning my in turning turning me me advice in been in my in my in some\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 2.0099  \n",
            "Epoch 11/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.7714\n",
            "--- SAMPLE TEXT AFTER EPOCH 11 ---\n",
            "\n",
            "        gatsby said over my that in in my some advice some my been i’ve my been turning over in in over my in my me my me me advice advice turning been in over my been turning in in my my me me in me my me me some that advice in\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 1.7714  \n",
            "Epoch 12/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.6153\n",
            "--- SAMPLE TEXT AFTER EPOCH 12 ---\n",
            "\n",
            "        gatsby said over in my me that turning my my over i’ve over turning in my my over turning over my my me me some i’ve my over over turning in my turning my my over me my some advice me turning over over in my in my over my some me\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 1.6153  \n",
            "Epoch 13/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.5244\n",
            "--- SAMPLE TEXT AFTER EPOCH 13 ---\n",
            "\n",
            "        gatsby said advice over my over in my that over my been my over my over turning over my my some in some my some been turning turning turning over my my my over over in my some some i’ve advice been in my that over over my in my my me\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 1.5244  \n",
            "Epoch 14/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.4491\n",
            "--- SAMPLE TEXT AFTER EPOCH 14 ---\n",
            "\n",
            "        gatsby said some i’ve my turning over my over over my my over my some some turning over over my i’ve my turning turning over over in my my over turning my my me that me i’ve been turning over my turning my my over over in my me some some been\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 1.4491  \n",
            "Epoch 15/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.3807\n",
            "--- SAMPLE TEXT AFTER EPOCH 15 ---\n",
            "\n",
            "        gatsby said that my been over my over over my over over my my over my me some i’ve been over my turning over my turning my over my over over over my my me my that i’ve turning my turning over over in my my my over some me over i’ve\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 1.3807  \n",
            "Epoch 16/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.2858\n",
            "--- SAMPLE TEXT AFTER EPOCH 16 ---\n",
            "\n",
            "        gatsby said over my i’ve turning over my over my over my turning over my over my that over over my over in my some over my my me i’ve i’ve turning over over my over my over my my over my some some i’ve been over over my over my over\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 1.2858  \n",
            "Epoch 17/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.2310\n",
            "--- SAMPLE TEXT AFTER EPOCH 17 ---\n",
            "\n",
            "        gatsby said i’ve my been over over my turning over my over my my over my some some been over that my been my over over my turning over my over over my my my me some over that that over over over my my my in my been over advice that\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 1.2310  \n",
            "Epoch 18/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.1764\n",
            "--- SAMPLE TEXT AFTER EPOCH 18 ---\n",
            "\n",
            "        gatsby said over my some turning over my over my over my over over my my some some that turning over my over my turning over my over my my some i’ve i’ve over turning my turning over over my my over my my some some over advice been over my over\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 1.1764  \n",
            "Epoch 19/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.1321\n",
            "--- SAMPLE TEXT AFTER EPOCH 19 ---\n",
            "\n",
            "        gatsby said over my some been my over my over over my over over my over my my me over some i’ve my turning over my over over my my over my me over over my that my some been over over my my over over my over my my me some\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 1.1321  \n",
            "Epoch 20/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.1022\n",
            "--- SAMPLE TEXT AFTER EPOCH 20 ---\n",
            "\n",
            "        gatsby said over my me turning my over my some that over my over in my me my some i’ve turning over over my over over in my my my me some some some that been turning turning turning over my in my my over my me some some that i’ve been\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 1.1022  \n",
            "Epoch 21/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.0527\n",
            "--- SAMPLE TEXT AFTER EPOCH 21 ---\n",
            "\n",
            "        gatsby said over my advice over my over my over over my my me that over turning my over my over over my in my me me me my some i’ve been been over turning in my my my over my me some me some advice been turning over my over in\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 1.0527  \n",
            "Epoch 22/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.9991\n",
            "--- SAMPLE TEXT AFTER EPOCH 22 ---\n",
            "\n",
            "        gatsby said over in my me advice over my my some turning my i’ve turning over over my over over my my over my my me me me some i’ve been turning in over my my my over over my me me some i’ve been over my in my turning my over\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 0.9991  \n",
            "Epoch 23/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.9888\n",
            "--- SAMPLE TEXT AFTER EPOCH 23 ---\n",
            "\n",
            "        gatsby said over my some i’ve over my over my turning my over over my over my me some over my advice my turning over my over my over my me over over my my some advice been over my over my over turning my my over my me me some over\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 0.9888  \n",
            "Epoch 24/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.9317\n",
            "--- SAMPLE TEXT AFTER EPOCH 24 ---\n",
            "\n",
            "        gatsby said over my some over my my me some i’ve turning over over my my over my my over over my me me some some turning over my my over over my my over over my me some some my advice been over my over over my my over my my\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 0.9317  \n",
            "Epoch 25/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.8777\n",
            "--- SAMPLE TEXT AFTER EPOCH 25 ---\n",
            "\n",
            "        gatsby said i’ve over my over my over my me over my over me some my advice over my my advice been turning over over over my my over my my my me me some some some been turning turning over my my over my my over my me me me some\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 0.8777  \n",
            "Epoch 26/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.8105\n",
            "--- SAMPLE TEXT AFTER EPOCH 26 ---\n",
            "\n",
            "        gatsby said over my some my some turning over my my me over over my over my my me some some my i’ve turning turning over over my my over my me my me some advice turning been over in my my over my over my me me some some turning over\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 0.8105  \n",
            "Epoch 27/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.7705\n",
            "--- SAMPLE TEXT AFTER EPOCH 27 ---\n",
            "\n",
            "        gatsby said over my me over my some my some over my advice i’ve over my turning over my my over over my me my me some some been over my over over my my my over my me me me advice advice i’ve turning turning in my my over my over\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 0.7705  \n",
            "Epoch 28/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.7308\n",
            "--- SAMPLE TEXT AFTER EPOCH 28 ---\n",
            "\n",
            "        gatsby said over my me over my some my some that over my turning over my over my over my my me me me some some over i’ve my turning over my my over my over me me some my advice over my my some i’ve turning over over my my over\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 0.7308  \n",
            "Epoch 29/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.6627\n",
            "--- SAMPLE TEXT AFTER EPOCH 29 ---\n",
            "\n",
            "        gatsby said over my me my me some been over my over my some over my my me me some advice over over my my over my my some advice over my me some that over over my my my over over my my me me me some advice turning my over\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 0.6627  \n",
            "Epoch 30/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.6226\n",
            "--- SAMPLE TEXT AFTER EPOCH 30 ---\n",
            "\n",
            "        gatsby said over my me over my me my me some advice over over my my some over my my me me some advice i’ve turning over my over my my over my me me me some some advice turning in my my my over over my me me me me some\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 0.6226  \n",
            "\n",
            "Final Synthesized Text:\n",
            "\n",
            "        gatsby said over my me over my me my some advice over my some i’ve over over my my my me me some over my some some been over my my over my over me my me some some over my my some i’ve turning over my my over me me over my my me some some over my in my me some advice my that over my me been over my over my me my me some over my some advice i’ve over my over my my over over my me me me advice my some over my over over my\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}